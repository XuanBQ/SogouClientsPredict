# coding=utf-8
import numpy as np
import jieba

from sklearn import metrics
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_extraction.text import CountVectorizer  # sklearn中的文本特征提取组件中，导入特征向量计数函数
from sklearn.feature_extraction.text import TfidfTransformer  # sklearn中的文本特征提取组件中，导入词频统计函数
from sklearn.naive_bayes import *
from sklearn import tree
from sklearn.cross_validation import train_test_split
from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest
from sklearn import svm
from sklearn.svm import libsvm
import matplotlib.pyplot as plt
from numpy import arange

import TextFeatureSelection
import time

# load data
def FetchDataset(filename):
    labels = []
    texts  = []
    terms  = []
    file   = open(filename)
    for line in file:
        l,t = line.split(' ', 1)
        segs = t.decode('utf-8').split()
        #segs = jieba.lcut(t, cut_all = False)
        labels.append(l)
        terms.append(segs)
        #text = ' '.join(segs)
        texts.append(t.strip())
    return terms, texts, labels


def classify(clf, train_X, train_Y, test_X, test_Y):
    #print 'train_X.shape = ', train_X.shape
    #print '--------------------------------------'
    clf.fit(train_X, train_Y)
    print clf
    print '--------------------------------------'
    predicted_Y = clf.predict(test_X)
    print(metrics.classification_report(test_Y, predicted_Y))
    return np.mean(predicted_Y == test_Y)


def sel_feat(train_X, train_Y, test_X, K):
    ch2 = SelectKBest(chi2, k = K)
    sel_train_X = ch2.fit_transform(train_X, train_Y)
    sel_test_X = ch2.transform(test_X)
    return sel_train_X, sel_test_X

def test_with_chi2(train_texts, train_targets, test_texts, test_targets, k_list):
    count_vect = CountVectorizer()
    tfidf_transformer = TfidfTransformer()  # 这里使用的是tf-idf
    X_train_counts = count_vect.fit_transform(train_texts)  # 对文本进行特征向量处理
    X_train_feats = tfidf_transformer.fit_transform(X_train_counts)
    X_test_counts = count_vect.transform(test_texts)  # 构建文档计数
    X_test_feats = tfidf_transformer.transform(X_test_counts)  # 构建文档tfidf

    accs = []
    for k in k_list:
        sel_train_X, sel_test_X  = sel_feat(X_train_feats, train_targets, X_test_feats, k)
        #print 'Features transformed, ', k
        acc = classify(clf, sel_train_X, train_targets, sel_test_X, test_targets)
        accs.append(acc)

    print accs
    return accs

def get_feats_with_chi2(train_texts, train_targets, test_texts, k = None):
    count_vect = CountVectorizer()
    tfidf_transformer = TfidfTransformer()  # 这里使用的是tf-idf
    X_train_counts = count_vect.fit_transform(train_texts)  # 对文本进行特征向量处理
    X_train_feats = tfidf_transformer.fit_transform(X_train_counts)
    X_test_counts = count_vect.transform(test_texts)  # 构建文档计数
    X_test_feats = tfidf_transformer.transform(X_test_counts)  # 构建文档tfidf
    if k != None:
        X_train_feats, X_test_feats  = sel_feat(X_train_feats, train_targets, X_test_feats, k)
    return X_train_feats, X_test_feats

# IG wllr mi
def sel_terms(train_X, train_Y, sel_feat_method = None, K = None):
    terms = sort_terms[:K]
    term_dict = dict(zip(terms, range(len(terms))))
    return term_dict

def feats(X_train_terms, X_train_texts, train_Y, X_test_texts, sel_feat_method = None, K = None):
    count_vect = CountVectorizer()
    tfidf_transformer = TfidfTransformer()  # 这里使用的是tf-idf

    if sel_feat_method != None:
        term_dict = sel_terms(X_train_terms, train_Y, sel_feat_method, K)
        count_vect.fixed_vocabulary = True
        count_vect.vocabulary_ = term_dict

    X_train_counts = count_vect.transform(X_train_texts)
    X_train_feats = tfidf_transformer.fit_transform(X_train_counts)
    X_test_counts = count_vect.transform(X_test_texts)  # 构建文档计数
    X_test_feats = tfidf_transformer.transform(X_test_counts)  # 构建文档tfidf

    return X_train_feats, X_test_feats


#SVM
#clf = SVC()
#clf.fit(X_train_tfidf, train_Y)

def test_NB(train_X, train_Y, test_X, test_Y):
    alpha_list = arange(0.01, 1.01, 0.01)
    NB_list = ('MultinomialNB', 'BernoulliNB')
    NB_acc_dict = {}
    for nb in NB_list:
        print 'NB: ', nb
        print '--------------------------------'
        if nb == 'MultinomialNB':
            NB = MultinomialNB()
        else:
            NB = BernoulliNB()
        acc_list = []
        for a in alpha_list:
            NB.alpha = a
            acc = classify(NB, train_X, train_Y, test_X, test_Y)
            acc_list.append(acc)
            print a, acc
        print acc_list
        NB_acc_dict[nb] = acc_list

    for nb in NB_list:
        plt.plot(alpha_list, NB_acc_dict[nb],  '--^',  label = nb)
        plt.title('NaiveBayesian - Edu')
        plt.xlabel('alpha')
        plt.ylabel('accuracy')
        plt.ylim((0.45, 0.65))

    plt.legend(loc='lower right', numpoints = 1)
    plt.show()

# KNN
#clf = KNeighborsClassifier(n_neighbors = 60, weights='distance')
#clf.fit(X_train_tfidf, train_Y)

def KNN(train_X, train_Y, test_X, test_Y, K_list):
    knn = KNeighborsClassifier(weights='distance')
    knn_acc_list = []
    for K in K_list:
        knn.n_neighbors = K
        # knn_acc_list = []
        acc = classify(knn, train_X, train_Y, test_X, test_Y)
        knn_acc_list.append(acc)
        print K, acc
        # print knn_acc_list

    return  knn_acc_list
    plt.plot(K_list, knn_acc_list,  '--^',  label = K)
    plt.title('KNN')
    plt.xlabel('K')
    plt.ylabel('accuracy')
    plt.ylim((0.45, 0.65))

    plt.legend(loc='lower right', numpoints = 1)
    plt.show()




# decision tree
#tree_model = tree.DecisionTreeClassifier(criterion='entropy')
#tree_model.fit(X_train_tfidf, train_Y)
def test_fs():
    time0 = time.clock()
    fs_method_list = ['CHI2','IG','WLLR']
    fs_num_list    = range(1000, 340000, 1000)
    acc_dict       = {}
    for method in fs_method_list:
        acc_list = []
        print '-------------------------------------'
        print 'Feature Selection Method: ', method
        if method == 'CHI2':
            acc_dict[method] = test_with_chi2(train_texts, train_targets, test_texts, test_targets, fs_num_list)
            #acc_dict[method]  = [0.79330372394943627, 0.79706183805944653, 0.80457806627946704, 0.8032114793303724, 0.80696959344038266, 0.80731124017765632, 0.80799453365220364, 0.80765288691492998, 0.80594465322856168, 0.8083361803894773, 0.80799453365220364, 0.80594465322856168, 0.80355312606764606, 0.80184489238127776, 0.8032114793303724, 0.8015032456440041, 0.80184489238127776, 0.7997950119576358, 0.80081995216945678, 0.80218653911855142, 0.80184489238127776, 0.80252818585582508, 0.80184489238127776, 0.80252818585582508, 0.8032114793303724, 0.80355312606764606, 0.80423641954219338, 0.80355312606764606, 0.8032114793303724, 0.80457806627946704, 0.80457806627946704, 0.80389477280491972, 0.80286983259309874, 0.80423641954219338, 0.80355312606764606, 0.80355312606764606, 0.80252818585582508, 0.80286983259309874, 0.8032114793303724, 0.80286983259309874, 0.80389477280491972, 0.80389477280491972, 0.8032114793303724, 0.80286983259309874, 0.80286983259309874, 0.8015032456440041, 0.80355312606764606, 0.80423641954219338, 0.80560300649128802, 0.80457806627946704, 0.80457806627946704, 0.80560300649128802, 0.80389477280491972, 0.80389477280491972, 0.80457806627946704, 0.8032114793303724, 0.80286983259309874, 0.80081995216945678, 0.8015032456440041, 0.80184489238127776, 0.80047830543218312, 0.80116159890673044, 0.80116159890673044, 0.80081995216945678, 0.8015032456440041, 0.80184489238127776, 0.8015032456440041, 0.8015032456440041, 0.80081995216945678, 0.80184489238127776, 0.80184489238127776, 0.80286983259309874, 0.80218653911855142, 0.80286983259309874, 0.80286983259309874, 0.8015032456440041, 0.80184489238127776, 0.80081995216945678, 0.80047830543218312, 0.80013665869490946, 0.80116159890673044, 0.80116159890673044, 0.80116159890673044, 0.80184489238127776, 0.80184489238127776, 0.8015032456440041, 0.8015032456440041, 0.80116159890673044, 0.80081995216945678, 0.80116159890673044, 0.8015032456440041, 0.80184489238127776, 0.8015032456440041, 0.80218653911855142, 0.80252818585582508, 0.80252818585582508, 0.80286983259309874, 0.80116159890673044, 0.80047830543218312, 0.80047830543218312, 0.8015032456440041, 0.80184489238127776, 0.80218653911855142, 0.80047830543218312, 0.80081995216945678, 0.80081995216945678, 0.80047830543218312, 0.80047830543218312, 0.80047830543218312, 0.80081995216945678, 0.8015032456440041, 0.8015032456440041, 0.80116159890673044, 0.80184489238127776, 0.8015032456440041, 0.80116159890673044, 0.80116159890673044, 0.8015032456440041, 0.80047830543218312, 0.80081995216945678, 0.80116159890673044, 0.8015032456440041, 0.8015032456440041, 0.80081995216945678, 0.8015032456440041, 0.80184489238127776, 0.80184489238127776, 0.80184489238127776, 0.8015032456440041, 0.8015032456440041, 0.8015032456440041, 0.80218653911855142, 0.80252818585582508, 0.80218653911855142, 0.80184489238127776, 0.8015032456440041, 0.8015032456440041, 0.8015032456440041, 0.8015032456440041, 0.80116159890673044, 0.80116159890673044, 0.80184489238127776, 0.80218653911855142, 0.8015032456440041, 0.80116159890673044, 0.80081995216945678, 0.80081995216945678, 0.80013665869490946, 0.80116159890673044]
        elif method == 'IG':
            acc_dict[method] = [0.60129825760163991, 0.61940553467714388, 0.6276050563717116, 0.63375469764263748, 0.64537068670994191, 0.64639562692176289, 0.65493679535360438, 0.65220362145541511, 0.6539118551417834, 0.66040314314998294, 0.66450290399726686, 0.67782712675093948, 0.67816877348821314, 0.68124359412367608, 0.68568500170823365, 0.68431841475913902, 0.68431841475913902, 0.68875982234369659, 0.69046805603006489, 0.69183464297915953, 0.69149299624188587, 0.68807652886914927, 0.69251793645370685, 0.69627605056371711, 0.69798428425008541, 0.69866757772463273, 0.69969251793645371, 0.70071745814827469, 0.70686709941920056, 0.71438332763922108, 0.71711650153741036, 0.72121626238469427, 0.72121626238469427, 0.72565766996925174, 0.72360778954560989, 0.72839084386744102, 0.73044072429108298, 0.72907413734198834, 0.72975743081653566, 0.7277075503928937, 0.729415784079262, 0.72839084386744102, 0.7311240177656303, 0.72804919713016736, 0.72839084386744102, 0.729415784079262, 0.73078237102835664, 0.73385719166381957, 0.73351554492654591, 0.73624871882473519, 0.73590707208746153, 0.73966518619747179, 0.74034847967201911, 0.74103177314656643, 0.74376494704475571, 0.74752306115476597, 0.74752306115476597, 0.74820635462931329, 0.75264776221387086, 0.75264776221387086, 0.75298940895114452, 0.75469764263751282, 0.75435599590023916, 0.75503928937478648, 0.75333105568841818, 0.75162282200204988, 0.75025623505295524, 0.75367270242569184, 0.75538093611206014, 0.7557225828493338, 0.75811411001024942, 0.75708916979842844, 0.76050563717116504, 0.76050563717116504, 0.7608472839084387, 0.76153057738298602, 0.76050563717116504, 0.76016399043389138, 0.75708916979842844, 0.75538093611206014, 0.75503928937478648, 0.75777246327297576, 0.75982234369661772, 0.76118893064571236, 0.76016399043389138, 0.76187222412025968, 0.7642637512811753, 0.76494704475572262, 0.76289716433208066, 0.76153057738298602, 0.7608472839084387, 0.76221387085753334, 0.762555517594807, 0.76392210454390164, 0.76460539801844896, 0.76563033823026994, 0.76392210454390164, 0.76392210454390164, 0.76494704475572262, 0.76392210454390164, 0.76358045780662798, 0.76460539801844896, 0.76494704475572262, 0.76733857191663823, 0.76563033823026994, 0.76392210454390164, 0.76392210454390164, 0.76289716433208066, 0.76323881106935432, 0.76323881106935432, 0.76358045780662798, 0.76392210454390164, 0.76289716433208066, 0.76153057738298602, 0.76016399043389138, 0.76050563717116504, 0.76221387085753334, 0.76118893064571236, 0.76153057738298602, 0.76118893064571236, 0.76118893064571236, 0.76289716433208066, 0.76323881106935432, 0.7642637512811753, 0.76392210454390164, 0.76392210454390164, 0.762555517594807, 0.76221387085753334, 0.762555517594807, 0.76187222412025968, 0.76187222412025968, 0.76392210454390164, 0.76221387085753334, 0.76392210454390164, 0.7642637512811753, 0.76153057738298602, 0.762555517594807, 0.76289716433208066, 0.76289716433208066, 0.76460539801844896, 0.76460539801844896, 0.76528869149299628, 0.76460539801844896, 0.76938845234028019, 0.76973009907755385, 0.76973009907755385, 0.77041339255210117, 0.77109668602664849, 0.77109668602664849, 0.77212162623846947, 0.77314656645029045, 0.77075503928937483, 0.77246327297574313, 0.77348821318756411, 0.77417150666211143, 0.77417150666211143, 0.77553809361120596, 0.77622138708575328, 0.77656303382302694, 0.77485480013665875, 0.77382985992483777, 0.77348821318756411, 0.77348821318756411, 0.77451315339938509, 0.77417150666211143, 0.77314656645029045, 0.77212162623846947, 0.77246327297574313, 0.77246327297574313, 0.77348821318756411, 0.77417150666211143, 0.77417150666211143, 0.77451315339938509, 0.77553809361120596, 0.77587974034847962, 0.7769046805603006, 0.7769046805603006, 0.7769046805603006, 0.77827126750939524, 0.77963785445848988, 0.77895456098394256, 0.7786129142466689, 0.77792962077212158, 0.77827126750939524, 0.78134608814485818, 0.77963785445848988, 0.77929620772121622, 0.78100444140758452, 0.77963785445848988, 0.77792962077212158, 0.77963785445848988, 0.77963785445848988, 0.77827126750939524, 0.77724632729757426, 0.77724632729757426, 0.7786129142466689, 0.77827126750939524, 0.77792962077212158, 0.77997950119576354, 0.78168773488213183, 0.78066279467031086, 0.7803211479330372, 0.78168773488213183, 0.77997950119576354, 0.77963785445848988, 0.77997950119576354, 0.78134608814485818, 0.78066279467031086, 0.77963785445848988, 0.7803211479330372, 0.78066279467031086, 0.78271267509395281, 0.78202938161940549, 0.78202938161940549, 0.78202938161940549, 0.78510420225486843, 0.78339596856850013, 0.78373761530577379, 0.78510420225486843, 0.78578749572941575, 0.78578749572941575, 0.78647078920396307, 0.78749572941578405, 0.78783737615305771, 0.78647078920396307, 0.78647078920396307, 0.78715408267851039, 0.78749572941578405, 0.78749572941578405, 0.78647078920396307, 0.78544584899214209, 0.78544584899214209, 0.78407926204304745, 0.78407926204304745, 0.78510420225486843, 0.78407926204304745, 0.78407926204304745, 0.78476255551759477, 0.78339596856850013, 0.78339596856850013, 0.78305432183122647, 0.78237102835667915, 0.78442090878032111, 0.78544584899214209, 0.78339596856850013, 0.78476255551759477, 0.78510420225486843, 0.78407926204304745, 0.78749572941578405, 0.78715408267851039, 0.78817902289033137, 0.78852066962760503, 0.78988725657669967, 0.78852066962760503, 0.78681243594123673, 0.78783737615305771, 0.78817902289033137, 0.78920396310215235, 0.78886231636487869, 0.78988725657669967, 0.78988725657669967, 0.78920396310215235, 0.79022890331397333, 0.79159549026306797, 0.79159549026306797, 0.79091219678852065, 0.79022890331397333, 0.78886231636487869, 0.78954560983942601, 0.78988725657669967, 0.79057055005124699, 0.78920396310215235, 0.79125384352579431, 0.79125384352579431, 0.79091219678852065, 0.79159549026306797, 0.79193713700034163, 0.79159549026306797, 0.79193713700034163, 0.79193713700034163, 0.79125384352579431, 0.79330372394943627, 0.79398701742398359, 0.79501195763580457, 0.79501195763580457, 0.79467031089853091, 0.79467031089853091, 0.79535360437307823, 0.79672019132217287, 0.79706183805944653, 0.79774513153399385, 0.79569525111035189, 0.79569525111035189, 0.79706183805944653, 0.79603689784762555, 0.79672019132217287, 0.79672019132217287, 0.79603689784762555, 0.79535360437307823, 0.79501195763580457, 0.79535360437307823, 0.79501195763580457, 0.79569525111035189, 0.79672019132217287, 0.79535360437307823, 0.79603689784762555, 0.79774513153399385, 0.79603689784762555, 0.79637854458489921, 0.79603689784762555, 0.79706183805944653, 0.79740348479672019, 0.79774513153399385, 0.79706183805944653, 0.79877007174581482, 0.79877007174581482, 0.79877007174581482, 0.79740348479672019, 0.79808677827126751, 0.79637854458489921, 0.79740348479672019, 0.79740348479672019, 0.79808677827126751, 0.79774513153399385, 0.79774513153399385, 0.79808677827126751, 0.79740348479672019, 0.79706183805944653, 0.79706183805944653, 0.79808677827126751, 0.79945336522036214, 0.79945336522036214, 0.7997950119576358, 0.80047830543218312, 0.79911171848308848, 0.79842842500854116, 0.79911171848308848, 0.7997950119576358, 0.79945336522036214]
        elif method == 'WLLR':
            acc_dict[method] = [0.68944311581824391, 0.69251793645370685, 0.69251793645370685, 0.68978476255551757, 0.69251793645370685, 0.69730099077553809, 0.69764263751281175, 0.69832593098735907, 0.69627605056371711, 0.69969251793645371, 0.70345063204646396, 0.70515886573283226, 0.71404168090194742, 0.7133583874274001, 0.71711650153741036, 0.71643320806286304, 0.71609156132558938, 0.71814144174923134, 0.72155790912196793, 0.71711650153741036, 0.71609156132558938, 0.71199180047830546, 0.71609156132558938, 0.71711650153741036, 0.7167748548001367, 0.72053296891014695, 0.72531602323197808, 0.72702425691834638, 0.72702425691834638, 0.73522377861291421, 0.73795695251110349, 0.73795695251110349, 0.73829859924837715, 0.73829859924837715, 0.73898189272292447, 0.73932353946019813, 0.74342330030748205, 0.73966518619747179, 0.74171506662111375, 0.74103177314656643, 0.74103177314656643, 0.74069012640929277, 0.74342330030748205, 0.74410659378202937, 0.74376494704475571, 0.74786470789203963, 0.74786470789203963, 0.74718141441749231, 0.75025623505295524, 0.75025623505295524, 0.74957294157840793, 0.75025623505295524, 0.75162282200204988, 0.74923129484113427, 0.75333105568841818, 0.75845575674752308, 0.75777246327297576, 0.75982234369661772, 0.76221387085753334, 0.75982234369661772, 0.76187222412025968, 0.76187222412025968, 0.76358045780662798, 0.76187222412025968, 0.76289716433208066, 0.76153057738298602, 0.7608472839084387, 0.76118893064571236, 0.76153057738298602, 0.76323881106935432, 0.76460539801844896, 0.76221387085753334, 0.76358045780662798, 0.76392210454390164, 0.76460539801844896, 0.76563033823026994, 0.76665527844209092, 0.7659719849675436, 0.76187222412025968, 0.76016399043389138, 0.76358045780662798, 0.76494704475572262, 0.76665527844209092, 0.76528869149299628, 0.76733857191663823, 0.77007174581482751, 0.76733857191663823, 0.76938845234028019, 0.76973009907755385, 0.76836351212845921, 0.76733857191663823, 0.76870515886573287, 0.76802186539118555, 0.76973009907755385, 0.77075503928937483, 0.77143833276392215, 0.77075503928937483, 0.77109668602664849, 0.77007174581482751, 0.76938845234028019, 0.76733857191663823, 0.76733857191663823, 0.76802186539118555, 0.77007174581482751, 0.77007174581482751, 0.76938845234028019, 0.76938845234028019, 0.76938845234028019, 0.76938845234028019, 0.76904680560300653, 0.76802186539118555, 0.76768021865391189, 0.76563033823026994, 0.76665527844209092, 0.76494704475572262, 0.76528869149299628, 0.76460539801844896, 0.76392210454390164, 0.76460539801844896, 0.76528869149299628, 0.76631363170481726, 0.7659719849675436, 0.7659719849675436, 0.76563033823026994, 0.76631363170481726, 0.76699692517936457, 0.76733857191663823, 0.76733857191663823, 0.76733857191663823, 0.76733857191663823, 0.76802186539118555, 0.76631363170481726, 0.76665527844209092, 0.76870515886573287, 0.76973009907755385, 0.76870515886573287, 0.76938845234028019, 0.76938845234028019, 0.76973009907755385, 0.77177997950119581, 0.77075503928937483, 0.77143833276392215, 0.77246327297574313, 0.77348821318756411, 0.77348821318756411, 0.77382985992483777, 0.77553809361120596, 0.77622138708575328, 0.77587974034847962, 0.77553809361120596, 0.7751964468739323, 0.7751964468739323, 0.77451315339938509, 0.77451315339938509, 0.77587974034847962, 0.77587974034847962, 0.77553809361120596, 0.77622138708575328, 0.77622138708575328, 0.77553809361120596, 0.77587974034847962, 0.77656303382302694, 0.77622138708575328, 0.77656303382302694, 0.77724632729757426, 0.78066279467031086, 0.77997950119576354, 0.78066279467031086, 0.77929620772121622, 0.77997950119576354, 0.7803211479330372, 0.78168773488213183, 0.78271267509395281, 0.78271267509395281, 0.78202938161940549, 0.78066279467031086, 0.78066279467031086, 0.78134608814485818, 0.78168773488213183, 0.78305432183122647, 0.78271267509395281, 0.78305432183122647, 0.78202938161940549, 0.78305432183122647, 0.78271267509395281, 0.78168773488213183, 0.78168773488213183, 0.78134608814485818, 0.78271267509395281, 0.78305432183122647, 0.78339596856850013, 0.78305432183122647, 0.78271267509395281, 0.78134608814485818, 0.78066279467031086, 0.78339596856850013, 0.78134608814485818, 0.78100444140758452, 0.78100444140758452, 0.78305432183122647, 0.78476255551759477, 0.78373761530577379, 0.78373761530577379, 0.78305432183122647, 0.78305432183122647, 0.78271267509395281, 0.78271267509395281, 0.78237102835667915, 0.78271267509395281, 0.78305432183122647, 0.78407926204304745, 0.78612914246668941, 0.78510420225486843, 0.78476255551759477, 0.78544584899214209, 0.78647078920396307, 0.78612914246668941, 0.78476255551759477, 0.78442090878032111, 0.78612914246668941, 0.78612914246668941, 0.78681243594123673, 0.78783737615305771, 0.78988725657669967, 0.79125384352579431, 0.79091219678852065, 0.79125384352579431, 0.79057055005124699, 0.78954560983942601, 0.78852066962760503, 0.78817902289033137, 0.78783737615305771, 0.78510420225486843, 0.78510420225486843, 0.78715408267851039, 0.78681243594123673, 0.78647078920396307, 0.78647078920396307, 0.78647078920396307, 0.78647078920396307, 0.78647078920396307, 0.78612914246668941, 0.78578749572941575, 0.78749572941578405, 0.78715408267851039, 0.78886231636487869, 0.78920396310215235, 0.78817902289033137, 0.78920396310215235, 0.78954560983942601, 0.79022890331397333, 0.78988725657669967, 0.78886231636487869, 0.78954560983942601, 0.78852066962760503, 0.78954560983942601, 0.78988725657669967, 0.78886231636487869, 0.78988725657669967, 0.79022890331397333, 0.79091219678852065, 0.79057055005124699, 0.78988725657669967, 0.79125384352579431, 0.78954560983942601, 0.78954560983942601, 0.79022890331397333, 0.78988725657669967, 0.79057055005124699, 0.79125384352579431, 0.79159549026306797, 0.79262043047488895, 0.79330372394943627, 0.79296207721216261, 0.79296207721216261, 0.79296207721216261, 0.79398701742398359, 0.79398701742398359, 0.79432866416125725, 0.79467031089853091, 0.79467031089853091, 0.79535360437307823, 0.79637854458489921, 0.79603689784762555, 0.79637854458489921, 0.79535360437307823, 0.79501195763580457, 0.79603689784762555, 0.79774513153399385, 0.79740348479672019, 0.79774513153399385, 0.79706183805944653, 0.79467031089853091, 0.79706183805944653, 0.79740348479672019, 0.79774513153399385, 0.79637854458489921, 0.79467031089853091, 0.79603689784762555, 0.79740348479672019, 0.79706183805944653, 0.79603689784762555, 0.79569525111035189, 0.79672019132217287, 0.79637854458489921, 0.79706183805944653, 0.79637854458489921, 0.79672019132217287, 0.79706183805944653, 0.79740348479672019, 0.79774513153399385, 0.79911171848308848, 0.79774513153399385, 0.79774513153399385, 0.79945336522036214, 0.7997950119576358, 0.79945336522036214, 0.79945336522036214, 0.79842842500854116, 0.79740348479672019, 0.79774513153399385, 0.79842842500854116, 0.79842842500854116, 0.79877007174581482, 0.79945336522036214, 0.79911171848308848, 0.79877007174581482, 0.79911171848308848, 0.79945336522036214, 0.80013665869490946, 0.79945336522036214, 0.79877007174581482, 0.80116159890673044, 0.8015032456440041, 0.80013665869490946, 0.79945336522036214, 0.79911171848308848, 0.80047830543218312, 0.80081995216945678]
        #sort_terms = TextFeatureSelection.feature_selection(train_terms, train_targets, method)
        for k in fs_num_list:
            #sel_train_X, sel_test_X  = feats(train_terms, train_texts, train_targets, test_texts, method, k)
            #acc = classify(clf, sel_train_X, train_targets, sel_test_X, test_targets)
            print k, acc_dict[method][k]
            #acc_list.append(acc)
        #print acc_list
        #acc_dict[method] = acc_list
        #print '-------------------------------------'

    print 'time = ', time.clock() - time0

    for fs_method in fs_method_list:
        plt.plot(fs_num_list, acc_dict[fs_method],  '--^',  label = fs_method)
        plt.title('feature  selection')
        plt.xlabel('number of features')
        plt.ylabel('accuracy')
        plt.ylim((0.55, 0.83))

    plt.legend( loc='lower right', numpoints = 1)
    plt.show()


def test_knn():
    Class_list = ['gender', 'age', 'edu']
    knn_acc_dict = {}
    K_list = range(50, 1500, 30)
    #for Class in Class_list:
    #    train_terms, train_texts, train_targets = FetchDataset('train4_' + Class + '_tokens')
    #    test_terms, test_texts,  test_targets  = FetchDataset('test4_' + Class + '_tokens')

    #    sel_train_X, sel_test_X = get_feats_with_chi2(train_texts, train_targets, test_texts)

    #    print '-------------------------------------'
    #    print Class, 'Data loaded and transformed'

    #    acc_list = KNN(sel_train_X, train_targets, sel_test_X, test_targets, K_list)
    #    knn_acc_dict[Class] = acc_list
    #    print acc_list
    knn_acc_dict['gender'] = [0.74683976768021865, 0.7574308165357021, 0.76050563717116504, 0.76358045780662798, 0.76563033823026994, 0.76904680560300653, 0.77075503928937483, 0.77656303382302694, 0.77451315339938509, 0.77792962077212158, 0.77724632729757426, 0.77656303382302694, 0.77758797403484792, 0.77929620772121622, 0.7786129142466689, 0.7803211479330372, 0.7786129142466689, 0.78202938161940549, 0.78271267509395281, 0.78168773488213183, 0.78271267509395281, 0.78100444140758452, 0.78202938161940549, 0.78407926204304745, 0.78237102835667915, 0.78373761530577379, 0.78407926204304745, 0.78407926204304745, 0.78510420225486843, 0.78510420225486843, 0.78476255551759477, 0.78544584899214209, 0.78647078920396307, 0.78407926204304745, 0.78476255551759477, 0.78442090878032111, 0.78544584899214209, 0.78647078920396307, 0.78442090878032111, 0.78510420225486843, 0.78510420225486843, 0.78339596856850013, 0.78544584899214209, 0.78407926204304745, 0.78373761530577379, 0.78407926204304745, 0.78578749572941575, 0.78442090878032111, 0.78476255551759477]
    knn_acc_dict['age'] = [0.4796195652173913, 0.47214673913043476, 0.47995923913043476, 0.47792119565217389, 0.47554347826086957, 0.47316576086956524, 0.47622282608695654, 0.47758152173913043, 0.47282608695652173, 0.47010869565217389, 0.46942934782608697, 0.46773097826086957, 0.46637228260869568, 0.46535326086956524, 0.46229619565217389, 0.46297554347826086, 0.4639945652173913, 0.46229619565217389, 0.46161684782608697, 0.45957880434782611, 0.45923913043478259, 0.46025815217391303, 0.45754076086956524, 0.45686141304347827, 0.45482336956521741, 0.45108695652173914, 0.45040760869565216, 0.45074728260869568, 0.45074728260869568, 0.45142663043478259, 0.45040760869565216, 0.4483695652173913, 0.44802989130434784, 0.44802989130434784, 0.44701086956521741, 0.44463315217391303, 0.44497282608695654, 0.44497282608695654, 0.44293478260869568, 0.44259510869565216, 0.44259510869565216, 0.44191576086956524, 0.44157608695652173, 0.44123641304347827, 0.43953804347826086, 0.43851902173913043, 0.43817934782608697, 0.43614130434782611, 0.43614130434782611]
    knn_acc_dict['edu'] = [0.51763832157445233, 0.52617898254734496, 0.5235796509468994, 0.51800965466023019, 0.51689565540289639, 0.51429632380245083, 0.51503898997400666, 0.51838098774600816, 0.51652432231711842, 0.51206832528778312, 0.51132565911622729, 0.50649832900111402, 0.50352766431489049, 0.50204233197177872, 0.49721500185666545, 0.49684366877088748, 0.49424433717044186, 0.49313033791310806, 0.49238767174155218, 0.49053100631266244, 0.48496101002599329, 0.48458967694021537, 0.48310434459710361, 0.48087634608243596, 0.48087634608243596, 0.47864834756776831, 0.47864834756776831, 0.47679168213887857, 0.47753434831043445, 0.47419235053843295, 0.47419235053843295, 0.47270701819532118, 0.4708503527664315, 0.4708503527664315, 0.47233568510954327, 0.47307835128109915, 0.47122168585220942, 0.47233568510954327, 0.47159301893798738, 0.46936502042331973, 0.4708503527664315, 0.47159301893798738, 0.46787968808020797, 0.46862235425176385, 0.46862235425176385, 0.46862235425176385, 0.46862235425176385, 0.46787968808020797, 0.4663943557370962]

    for Class in Class_list:
        plt.plot(K_list, knn_acc_dict[Class],  '--^',  label = Class)
        plt.title('KNN')
        plt.xlabel('K')
        plt.ylabel('accuracy')
        plt.ylim((0.30, 0.90))

    plt.legend( loc='lower right', numpoints = 1)
    plt.show()

def test_models():
    Class_list = ['gender', 'age', 'edu']
    knn_clf    = KNeighborsClassifier(weights='distance')
    nb_clf     = BernoulliNB()
    clf_list   = [knn_clf, nb_clf]
    for Class in Class_list:
        train_terms, train_texts, train_targets = FetchDataset('train4_' + Class + '_tokens')
        test_terms, test_texts,  test_targets  = FetchDataset('test4_' + Class + '_tokens')

        sel_train_X, sel_test_X = get_feats_with_chi2(train_texts, train_targets, test_texts, 10000)

        print '-------------------------------------'
        print Class, 'Data loaded and transformed'

